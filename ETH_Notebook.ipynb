{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "13oEkgALWsIPHR4VRaoJ5ijQ460fKD0wz",
      "authorship_tag": "ABX9TyPMe/2DfdaO21imWyXZe9AJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/123RohitVarshit/123RohitVarshit/blob/main/ETH_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3Iql4N51AVf",
        "outputId": "bba1b554-adb5-4c56-f1a7-68c352d03608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pedagogicalrl'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 114 (delta 41), reused 69 (delta 20), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (114/114), 672.30 KiB | 4.20 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/eth-lre/pedagogicalrl.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pedagogicalrl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6Uf69pw1tQY",
        "outputId": "606742cb-9817-4c40-cd1a-0def64eb12d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pedagogicalrl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "h79DYFt-1w54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517c31f3-9bee-4ae8-af9e-756d517b9b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/eth-lre/pedagogicalrl.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI-LReJbGWL4",
        "outputId": "c1cf4343-7a7f-4b97-d5a6-950cbbf17097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pedagogicalrl'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 114 (delta 41), reused 69 (delta 20), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (114/114), 672.30 KiB | 3.46 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/pedagogicalrl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncb-pPwAGtKg",
        "outputId": "5fb6c13e-7cb3-4c1c-f3e7-c9a381bd3e71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/pedagogicalrl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install ONLY lightweight tools (No heavy torch/cuda downloads)\n",
        "!pip install hydra-core omegaconf python-dotenv openai google-generativeai colorama --quiet\n",
        "\n",
        "# This code creates a fake vLLM library in memory that matches the repo's imports exactly.\n",
        "import sys\n",
        "import types\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Any, Optional\n",
        "from importlib.machinery import ModuleSpec\n",
        "\n",
        "print(\"üõ°Ô∏è Initializing Robust vLLM Mocking...\")\n",
        "\n",
        "# Helper to create valid modules with specs (Satisfies 'transformers' checks)\n",
        "def mock_module(name):\n",
        "    m = types.ModuleType(name)\n",
        "    m.__spec__ = ModuleSpec(name=name, loader=None)\n",
        "    sys.modules[name] = m\n",
        "    return m\n",
        "\n",
        "# --- A. Mock 'vllm' Top-Level ---\n",
        "vllm = mock_module(\"vllm\")\n",
        "\n",
        "@dataclass\n",
        "class SamplingParams:\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 1.0\n",
        "    top_k: int = -1\n",
        "    max_tokens: int = 100\n",
        "    n: int = 1\n",
        "    logits_processors: Any = None\n",
        "    stop: Optional[List[str]] = None\n",
        "\n",
        "@dataclass\n",
        "class CompletionOutput:\n",
        "    index: int\n",
        "    text: str\n",
        "    token_ids: List[int]\n",
        "    cumulative_logprob: float\n",
        "    logprobs: List[Any]\n",
        "\n",
        "@dataclass\n",
        "class RequestOutput:\n",
        "    request_id: str\n",
        "    prompt: str\n",
        "    outputs: List[CompletionOutput]\n",
        "    prompt_token_ids: List[int]\n",
        "    prompt_logprobs: List[Any]\n",
        "    finished: bool\n",
        "\n",
        "class PoolingOutput:\n",
        "    pass\n",
        "\n",
        "class LLM:\n",
        "    def __init__(self, *args, **kwargs): pass\n",
        "    def encode(self, *args, **kwargs): return []\n",
        "    def chat(self, *args, **kwargs): return []\n",
        "\n",
        "vllm.SamplingParams = SamplingParams\n",
        "vllm.CompletionOutput = CompletionOutput\n",
        "vllm.RequestOutput = RequestOutput\n",
        "vllm.PoolingOutput = PoolingOutput\n",
        "vllm.LLM = LLM\n",
        "\n",
        "# --- B. Mock 'vllm.config' (for pedagogical_reward.py) ---\n",
        "vllm_config = mock_module(\"vllm.config\")\n",
        "\n",
        "class PoolerConfig:\n",
        "    def __init__(self, pooling_type, **kwargs): pass\n",
        "\n",
        "vllm_config.PoolerConfig = PoolerConfig\n",
        "vllm.config = vllm_config\n",
        "\n",
        "# --- C. Mock 'vllm.distributed.parallel_state' (for data_parallel_vllm.py) ---\n",
        "vllm_dist = mock_module(\"vllm.distributed\")\n",
        "vllm_dist_ps = mock_module(\"vllm.distributed.parallel_state\")\n",
        "\n",
        "def dummy_destroy(): pass\n",
        "vllm_dist_ps.destroy_model_parallel = dummy_destroy\n",
        "vllm_dist_ps.destroy_distributed_environment = dummy_destroy\n",
        "\n",
        "vllm_dist.parallel_state = vllm_dist_ps\n",
        "vllm.distributed = vllm_dist\n",
        "\n",
        "# --- D. Mock 'liger_kernel' & 'deepspeed' (for trainer imports) ---\n",
        "mock_module(\"liger_kernel\")\n",
        "mock_module(\"liger_kernel.chunked_loss\")\n",
        "mock_module(\"deepspeed\")\n",
        "\n",
        "print(\"‚úÖ vLLM, DeepSpeed, and Liger Kernel successfully mocked.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lGvcShZFKfc",
        "outputId": "1fd42c82-1a15-41cd-cce2-a87698be25ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/154.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hüõ°Ô∏è Initializing Robust vLLM Mocking...\n",
            "‚úÖ vLLM, DeepSpeed, and Liger Kernel successfully mocked.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. TOPOLOGY LOGIC (Genotype)\n",
        "code_topology = \"\"\"\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "class Action:\n",
        "    DIAGNOSE = \"diagnose\"\n",
        "    SCAFFOLD = \"scaffold\"\n",
        "    HINT = \"hint\"\n",
        "    VERIFY = \"verify\"\n",
        "    ENCOURAGE = \"encourage\"\n",
        "\n",
        "    @staticmethod\n",
        "    def all(): return [Action.DIAGNOSE, Action.SCAFFOLD, Action.HINT, Action.VERIFY, Action.ENCOURAGE]\n",
        "\n",
        "@dataclass\n",
        "class Topology:\n",
        "    genes: List[str]\n",
        "    fitness: float = -999.0\n",
        "\n",
        "    def get_instruction(self, turn_idx: int) -> str:\n",
        "        if turn_idx >= len(self.genes): return \"Guide the student gently.\"\n",
        "        step = self.genes[turn_idx]\n",
        "        prompts = {\n",
        "            Action.DIAGNOSE: \"Do NOT solve. Ask the student what they think the first step is.\",\n",
        "            Action.SCAFFOLD: \"Break the problem down. Create a simpler example with different numbers.\",\n",
        "            Action.HINT: \"Give a conceptual hint about the formula, but do NOT mention the numbers.\",\n",
        "            Action.VERIFY: \"Ask the student to double-check their last calculation.\",\n",
        "            Action.ENCOURAGE: \"Tell them they are making progress, but ask them to try the step again.\"\n",
        "        }\n",
        "        return prompts.get(step, \"Guide the student.\")\n",
        "\n",
        "    def mutate(self):\n",
        "        idx = random.randint(0, len(self.genes)-1)\n",
        "        self.genes[idx] = random.choice(Action.all())\n",
        "\n",
        "    @classmethod\n",
        "    def crossover(cls, p1, p2):\n",
        "        if len(p1.genes) < 2: return cls(genes=p1.genes)\n",
        "        split = random.randint(1, len(p1.genes)-1)\n",
        "        return cls(genes=p1.genes[:split] + p2.genes[split:])\n",
        "\n",
        "    @classmethod\n",
        "    def random_init(cls, length=4):\n",
        "        return cls(genes=random.choices(Action.all(), k=length))\n",
        "\"\"\"\n",
        "with open(\"src/topology.py\", \"w\") as f: f.write(code_topology)\n",
        "\n",
        "# 2. CLASSROOM WRAPPER (Phenotype)\n",
        "code_top_class = \"\"\"\n",
        "from src.classroom import Conversation, ConversationState\n",
        "from src.topology import Topology\n",
        "from jinja2 import Template\n",
        "\n",
        "class TopologyConversation(Conversation):\n",
        "    def __init__(self, problem, answer, generation_cfg, topology: Topology):\n",
        "        super().__init__(problem, answer, generation_cfg)\n",
        "        self.topology = topology\n",
        "        self.turn_count = 0\n",
        "        self.template = Template(\n",
        "            \"TASK: Math Tutor.\\\\n\"\n",
        "            \"STRATEGY: {{instruction}}\\\\n\"\n",
        "            \"PROBLEM: {{problem}}\\\\n\"\n",
        "            \"HISTORY: {{history}}\\\\n\"\n",
        "            \"RESPONSE:\"\n",
        "        )\n",
        "\n",
        "    def get_conversation(self):\n",
        "        if self.state == ConversationState.TEACHER_TURN:\n",
        "            instruction = self.topology.get_instruction(self.turn_count)\n",
        "            hist = \"\\\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in self.conversation[-4:]])\n",
        "            prompt = self.template.render(problem=self.problem, instruction=instruction, history=hist)\n",
        "            self.turn_count += 1\n",
        "            return [{\"role\": \"user\", \"content\": prompt}]\n",
        "        return super().get_conversation()\n",
        "\"\"\"\n",
        "with open(\"src/topology_classroom.py\", \"w\") as f: f.write(code_top_class)\n",
        "\n",
        "# 3. EVOLUTION RUNNER (Main Script)\n",
        "code_runner = \"\"\"\n",
        "# RE-INJECT MOCKS FOR THE SCRIPT PROCESS\n",
        "import sys, types\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Any\n",
        "from importlib.machinery import ModuleSpec\n",
        "\n",
        "def mock_module(name):\n",
        "    m = types.ModuleType(name)\n",
        "    m.__spec__ = ModuleSpec(name=name, loader=None)\n",
        "    sys.modules[name] = m\n",
        "    return m\n",
        "\n",
        "if \"vllm\" not in sys.modules:\n",
        "    vllm = mock_module(\"vllm\")\n",
        "    @dataclass\n",
        "    class SamplingParams:\n",
        "        temperature: float = 0.7; top_p: float = 1.0; top_k: int = -1; max_tokens: int = 100; n: int = 1; logits_processors: Any = None; stop: Any = None\n",
        "    @dataclass\n",
        "    class CompletionOutput:\n",
        "        index: int; text: str; token_ids: List[int]; cumulative_logprob: float; logprobs: List[Any]\n",
        "    @dataclass\n",
        "    class RequestOutput:\n",
        "        request_id: str; prompt: str; outputs: List[CompletionOutput]; prompt_token_ids: List[int]; prompt_logprobs: List[Any]; finished: bool\n",
        "    class PoolingOutput: pass\n",
        "    class LLM:\n",
        "        def __init__(self, *args, **kwargs): pass\n",
        "        def encode(self, *args, **kwargs): return []\n",
        "        def chat(self, *args, **kwargs): return []\n",
        "\n",
        "    vllm.SamplingParams = SamplingParams; vllm.CompletionOutput = CompletionOutput\n",
        "    vllm.RequestOutput = RequestOutput; vllm.PoolingOutput = PoolingOutput; vllm.LLM = LLM\n",
        "\n",
        "    vllm_config = mock_module(\"vllm.config\")\n",
        "    class PoolerConfig:\n",
        "        def __init__(self, pooling_type, **kwargs): pass\n",
        "    vllm_config.PoolerConfig = PoolerConfig\n",
        "    vllm.config = vllm_config\n",
        "\n",
        "    vllm_dist = mock_module(\"vllm.distributed\")\n",
        "    vllm_dist_ps = mock_module(\"vllm.distributed.parallel_state\")\n",
        "    vllm_dist_ps.destroy_model_parallel = lambda: None\n",
        "    vllm_dist_ps.destroy_distributed_environment = lambda: None\n",
        "    vllm_dist.parallel_state = vllm_dist_ps\n",
        "    vllm.distributed = vllm_dist\n",
        "\n",
        "    mock_module(\"liger_kernel\")\n",
        "    mock_module(\"liger_kernel.chunked_loss\")\n",
        "    mock_module(\"deepspeed\")\n",
        "\n",
        "# --- ACTUAL SCRIPT STARTS HERE ---\n",
        "import hydra\n",
        "import random\n",
        "import copy\n",
        "import logging\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from src.classroom import Classroom, ConversationState\n",
        "from src.topology import Topology, Action\n",
        "from src.topology_classroom import TopologyConversation\n",
        "from config.eval import EvalConfig\n",
        "from hydra.core.config_store import ConfigStore\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
        "logger = logging.getLogger()\n",
        "\n",
        "cs = ConfigStore.instance()\n",
        "cs.store(name=\"config\", node=EvalConfig)\n",
        "\n",
        "def calculate_fitness(conv, ans, length):\n",
        "    hist = conv.conversation\n",
        "    st_msgs = [m['content'] for m in hist if m['role'] == 'student']\n",
        "    te_msgs = [m['content'] for m in hist if m['role'] == 'teacher']\n",
        "\n",
        "    if not st_msgs: return 0.0\n",
        "\n",
        "    success = str(ans) in st_msgs[-1]\n",
        "    score = 100.0 if success else 10.0\n",
        "    if success: score += (length - len(te_msgs)) * 15\n",
        "    for m in te_msgs:\n",
        "        if str(ans) in m: score -= 50.0\n",
        "    return score\n",
        "\n",
        "@hydra.main(config_path=\"config/eval\", version_base=None)\n",
        "def main(cfg: EvalConfig):\n",
        "    load_dotenv()\n",
        "    print(\"\\\\nüß¨ INITIALIZING EVOLUTIONARY TOPOLOGIES (SECURE MODE)...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    POP_SIZE = 4\n",
        "    GENERATIONS = 3\n",
        "    PROBLEM = \"Solve for x: 3x + 12 = 27\"\n",
        "    ANSWER = \"5\"\n",
        "\n",
        "    classroom = Classroom(cfg.student_model, cfg.teacher_model, cfg.judge_model, cfg.reward_model, cfg.generation, None)\n",
        "\n",
        "    pop = [Topology.random_init() for _ in range(POP_SIZE)]\n",
        "    best_score = -999\n",
        "\n",
        "    for gen in range(GENERATIONS):\n",
        "        print(f\"\\\\n‚ö° GENERATION {gen+1}/{GENERATIONS}\")\n",
        "        conversations = []\n",
        "        for dna in pop:\n",
        "            c = TopologyConversation(PROBLEM, ANSWER, cfg.generation, topology=dna)\n",
        "            c.start_conversation()\n",
        "            conversations.append(c)\n",
        "\n",
        "        for _ in range(4):\n",
        "            act_t = [c for c in conversations if c.state == ConversationState.TEACHER_TURN]\n",
        "            if act_t: classroom.generate_next_teacher_utterances(act_t)\n",
        "            act_s = [c for c in conversations if c.state == ConversationState.STUDENT_TURN]\n",
        "            if act_s: classroom.generate_next_student_utterances(act_s)\n",
        "\n",
        "        scores = []\n",
        "        for i, c in enumerate(conversations):\n",
        "            f = calculate_fitness(c, ANSWER, 4)\n",
        "            pop[i].fitness = f\n",
        "            scores.append(f)\n",
        "            status = \"‚úÖ Solved\" if f > 50 else \"‚ùå Failed\"\n",
        "            print(f\"   [Org {i}] {pop[i].genes} | Score: {f:.1f}\")\n",
        "\n",
        "        if max(scores) > best_score:\n",
        "            best_score = max(scores)\n",
        "\n",
        "        new_pop = []\n",
        "        while len(new_pop) < POP_SIZE:\n",
        "            p = random.choice(pop)\n",
        "            child = copy.deepcopy(p)\n",
        "            if random.random() < 0.6: child.mutate()\n",
        "            new_pop.append(child)\n",
        "        pop = new_pop\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*60)\n",
        "    print(\"üèÜ EVOLUTION COMPLETE\")\n",
        "    print(f\"Final Best Score: {best_score}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "with open(\"run_evolution.py\", \"w\") as f: f.write(code_runner)\n",
        "\n",
        "print(\"‚úÖ Research files created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TYaTU_0LZyM",
        "outputId": "e775b3df-c724-484f-99c2-6651453c78fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Research files created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "classroom_path = \"src/classroom.py\"\n",
        "\n",
        "print(f\"üîß Repairing {classroom_path}...\")\n",
        "\n",
        "with open(classroom_path, \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# FIX 1: Replace the broken \"self.getattr(generation_cfg\" with \"getattr(self.generation_cfg\"\n",
        "# This handles the specific error you saw in the traceback.\n",
        "if \"self.getattr(generation_cfg\" in content:\n",
        "    content = content.replace(\"self.getattr(generation_cfg\", \"getattr(self.generation_cfg\")\n",
        "    print(\"   -> Fixed 'self.getattr(generation_cfg' instances.\")\n",
        "\n",
        "# FIX 2: Catch cases where it might be \"self.getattr(self.generation_cfg\" (rare but possible artifact)\n",
        "if \"self.getattr(self.generation_cfg\" in content:\n",
        "    content = content.replace(\"self.getattr(self.generation_cfg\", \"getattr(self.generation_cfg\")\n",
        "    print(\"   -> Fixed 'self.getattr(self.generation_cfg' instances.\")\n",
        "\n",
        "# FIX 3: Ensure getattr is used correctly for 'max_turns' which might appear as self.getattr(generation_cfg...\n",
        "# We simply do a global replace for the pattern caused by the previous regex script.\n",
        "content = content.replace(\"self.getattr(\", \"getattr(self.\")\n",
        "\n",
        "# FIX 4: Re-verify local variables vs self attributes\n",
        "# If the previous script replaced 'generation_cfg.attr' (local var) with 'getattr(generation_cfg, ...)'\n",
        "# that is valid. We only want to remove 'self.' from BEFORE getattr.\n",
        "# The replace above (FIX 3) might have been too aggressive if 'self.getattr' wasn't followed by 'generation_cfg'.\n",
        "# Let's check specifically for the pattern in the traceback.\n",
        "\n",
        "# Reload content to be clean and do precise replacement\n",
        "with open(classroom_path, \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Precise Fixes based on your traceback\n",
        "# The bad pattern: self.getattr(generation_cfg, 'max_tokens_in_conversation', 8192)\n",
        "# The wanted pattern: getattr(self.generation_cfg, 'max_tokens_in_conversation', 8192)\n",
        "\n",
        "patched_content = content.replace(\"self.getattr(generation_cfg\", \"getattr(self.generation_cfg\")\n",
        "\n",
        "# Write back\n",
        "with open(classroom_path, \"w\") as f:\n",
        "    f.write(patched_content)\n",
        "    f.flush()\n",
        "    os.fsync(f.fileno())\n",
        "\n",
        "print(\"‚úÖ File repaired. The AttributeError should be resolved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7j58lcEG-yw",
        "outputId": "4e4fe14e-e1f5-4271-8571-d18575536ccd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Repairing src/classroom.py...\n",
            "   -> Fixed 'self.getattr(generation_cfg' instances.\n",
            "‚úÖ File repaired. The AttributeError should be resolved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acc6baa1",
        "outputId": "af17faba-5a8b-4045-b83e-511a689aefed"
      },
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Load Secrets Securely\n",
        "try:\n",
        "    # Try OpenRouter first\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = userdata.get('OPENROUTER_API_KEY')\n",
        "    print(\"‚úÖ Loaded OPENROUTER_API_KEY\")\n",
        "except:\n",
        "    try:\n",
        "        # Try Gemini\n",
        "        os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "        print(\"‚úÖ Loaded GEMINI_API_KEY\")\n",
        "    except:\n",
        "        print(\"‚ùå ERROR: Keys not found. Please set secrets in Colab sidebar.\")\n",
        "\n",
        "# 2. Run\n",
        "!python run_evolution.py \\\n",
        "  --config-name Qwen2.5-7B-Instruct.yaml \\\n",
        "  teacher_model.use_openrouter=True \\\n",
        "  teacher_model.model_name_or_path=\"meta-llama/llama-3.1-8b-instruct\" \\\n",
        "  +student_model.use_openrouter=True \\\n",
        "  student_model.model_name_or_path=\"meta-llama/llama-3.1-8b-instruct\" \\\n",
        "  +judge_model.use_openrouter=True \\\n",
        "  judge_model.model_name_or_path=\"meta-llama/llama-3.1-8b-instruct\" \\\n",
        "  +generation.number_judge_attempts=0"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded OPENROUTER_API_KEY\n",
            "\n",
            "üß¨ INITIALIZING EVOLUTIONARY TOPOLOGIES (SECURE MODE)...\n",
            "============================================================\n",
            "\n",
            "‚ö° GENERATION 1/3\n",
            "[2025-12-14 18:25:36,063][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:36,107][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:36,251][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:36,383][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:25:40,925][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:41,269][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:41,329][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:41,519][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:25:45,447][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:45,595][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:45,610][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:45,614][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:25:49,198][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:49,339][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:49,686][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:49,764][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:25:54,598][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:54,615][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:54,623][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:55,115][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:25:57,955][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:58,185][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:58,285][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:58,387][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:01,128][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:01,397][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:01,480][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:01,587][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "   [Org 0] ['diagnose', 'hint', 'scaffold', 'diagnose'] | Score: -40.0\n",
            "   [Org 1] ['encourage', 'hint', 'encourage', 'diagnose'] | Score: -90.0\n",
            "   [Org 2] ['encourage', 'hint', 'scaffold', 'diagnose'] | Score: 15.0\n",
            "   [Org 3] ['diagnose', 'diagnose', 'verify', 'diagnose'] | Score: 65.0\n",
            "\n",
            "‚ö° GENERATION 2/3\n",
            "[2025-12-14 18:26:03,171][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:03,510][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:03,593][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:03,644][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:06,398][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:06,475][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:06,696][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:06,791][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:07,936][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:08,266][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:08,273][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:08,377][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:10,479][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:10,631][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:10,689][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:10,727][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:15,183][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:15,573][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:15,749][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:16,041][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:17,211][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:17,442][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:17,641][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:17,708][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:23,346][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:23,461][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:23,641][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:23,646][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "   [Org 0] ['encourage', 'hint', 'scaffold', 'diagnose'] | Score: -35.0\n",
            "   [Org 1] ['diagnose', 'diagnose', 'scaffold', 'diagnose'] | Score: -40.0\n",
            "   [Org 2] ['encourage', 'hint', 'encourage', 'diagnose'] | Score: -90.0\n",
            "   [Org 3] ['diagnose', 'hint', 'scaffold', 'diagnose'] | Score: -90.0\n",
            "\n",
            "‚ö° GENERATION 3/3\n",
            "[2025-12-14 18:26:26,083][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:26,116][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:26,324][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:26,387][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:32,014][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:32,220][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:32,318][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:32,347][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:37,174][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:37,227][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:37,597][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:37,629][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:42,825][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:43,091][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:43,316][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:43,324][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:45,324][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:45,486][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:45,529][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:45,713][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:48,485][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:48,719][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:48,775][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:48,786][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:52,756][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:52,803][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:52,909][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:53,060][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "   [Org 0] ['diagnose', 'scaffold', 'scaffold', 'diagnose'] | Score: -90.0\n",
            "   [Org 1] ['verify', 'hint', 'encourage', 'diagnose'] | Score: -40.0\n",
            "   [Org 2] ['diagnose', 'hint', 'scaffold', 'diagnose'] | Score: 65.0\n",
            "   [Org 3] ['diagnose', 'diagnose', 'scaffold', 'diagnose'] | Score: 115.0\n",
            "\n",
            "============================================================\n",
            "üèÜ EVOLUTION COMPLETE\n",
            "Final Best Score: 115.0\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}