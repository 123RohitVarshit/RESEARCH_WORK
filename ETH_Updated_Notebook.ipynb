{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13oEkgALWsIPHR4VRaoJ5ijQ460fKD0wz",
      "authorship_tag": "ABX9TyOElfWUkkywILWPtud6neIG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/123RohitVarshit/RESEARCH_WORK/blob/main/ETH_Updated_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROJECT REPORT: EVOLUTIONARY PEDAGOGICAL TOPOLOGIES (EPT)\n",
        "# Research Prototype Extension for eth-lre/pedagogicalrl\n",
        "-------------------------------------------------------------------------\n",
        "\n",
        "1. EXECUTIVE SUMMARY & MOTIVATION\n",
        "\n",
        "Current Reinforcement Learning from Human Feedback (RLHF) approaches for pedagogical alignment often suffer from \"mode collapse.\" While they effectively optimize for a scalar reward (e.g., student success rate), the resulting models tend to converge to a single, repetitive teaching script. This lack of diversity limits the system's ability to adapt to different student learning styles.\n",
        "\n",
        "This project proposes and implements \"Evolutionary Pedagogical Topologies\" (EPT). The core innovation is the separation of the reasoning structure from the textual execution:\n",
        "\n",
        "* Genotype (Structure): An evolvable graph of pedagogical actions (e.g., Diagnose -> Scaffold -> Hint -> Verify).\n",
        "* Phenotype (Execution): The Natural Language Generation (NLG) performed by an LLM based on the Genotype's instructions.\n",
        "\n",
        "\n",
        "By evolving the structure offline using a Genetic Algorithm, this approach generates diverse, high-quality teaching strategies with significantly higher token efficiency than unstructured Chain-of-Thought prompting.\n",
        "\n",
        "-------------------------------------------------------------------------\n",
        "\n",
        "2. STATEMENT OF WORK: ARCHITECTURAL IMPLEMENTATION\n",
        "\n",
        "To demonstrate this logic within the constraints of a standard computational environment, I implemented a modular extension of the original 'pedagogicalrl' codebase. The implementation preserves the original class hierarchy while injecting evolutionary logic.\n",
        "\n",
        "2.1 The Genotype Module (src/topology.py)\n",
        "I defined a 'Topology' class that acts as the DNA for a teaching strategy.\n",
        "- Gene Definition: A sequence of pedagogical primitives: DIAGNOSE, SCAFFOLD, HINT, VERIFY, ENCOURAGE.\n",
        "- Evolutionary Operators: Implemented 'mutate' (single-point mutation) and 'crossover' (recombination of two parent topologies) to facilitate exploration of the strategy space.\n",
        "- Instruction Mapping: A deterministic mapping system that translates abstract genes into specific system prompts for the LLM.\n",
        "\n",
        "2.2 The Phenotype Wrapper (src/topology_classroom.py)\n",
        "I created 'TopologyConversation', a subclass inheriting from the repository's base 'Conversation' class.\n",
        "- Polymorphism: It overrides the 'get_conversation()' method.\n",
        "- Logic: Instead of allowing the LLM to generate freely, this class injects the specific instruction corresponding to the current turn's gene into the system prompt. This enforces the Genotype's structure onto the Phenotype's execution.\n",
        "\n",
        "2.3 The Evolutionary Engine (run_evolution.py)\n",
        "I developed a custom training loop to replace the standard RL trainer.\n",
        "- Fitness Function: A composite metric that rewards student correctness and conversation brevity, while strictly penalizing answer leakage.\n",
        "- Selection Mechanism: Implemented Fitness Proportional Selection (Roulette Wheel) with Elitism to preserve high-performing strategies.\n",
        "- Generalization: The engine evaluates each topology across multiple distinct algebraic problems to prevent overfitting to a single prompt.\n",
        "\n",
        "-------------------------------------------------------------------------\n",
        "\n",
        "3. TECHNICAL DEFENSE: ENGINEERING DECISIONS\n",
        "\n",
        "The original 'pedagogicalrl' repository is designed for High-Performance Computing (HPC) environments, with dependencies on vLLM, DeepSpeed, and Liger Kernel. These libraries require CUDA-capable GPUs and significant disk space (>10GB). To make this research prototype portable, accessible, and suitable for rapid iteration on standard hardware (Google Colab), I implemented the following engineering architecture:\n",
        "\n",
        "3.1 Runtime Mocking System\n",
        "Instead of modifying the codebase to remove imports (which breaks dependency chains), I implemented a 'sys.modules' interception system.\n",
        "- Mechanism: Before the main script runs, I inject dummy Python objects (types.ModuleType) into 'sys.modules' for 'vllm', 'deepspeed', and 'pynvml'.\n",
        "- Result: This tricks the Python interpreter into believing the heavy GPU libraries are installed, allowing the repository's logic to load without crashing, while consuming <50MB of data.\n",
        "\n",
        "3.2 API-Based Inference Adapter\n",
        "I bypassed the local weight loading mechanisms.\n",
        "- Implementation: Configured the experimental 'use_openrouter' flags within the Hydra configuration.\n",
        "- Justification: This routes the inference generation to external APIs (hosting Llama-3-8B). This allows the logic of the evolutionary algorithm to be validated using State-of-the-Art models without requiring local A100 GPUs.\n",
        "\n",
        "This approach demonstrates an ability to work within constraints and modify complex system architectures without breaking the underlying logic flow.\n",
        "\n",
        "-------------------------------------------------------------------------\n",
        "\n",
        "4. DEVELOPMENT LOG: CHALLENGES AND DEBUGGING\n",
        "\n",
        "Developing this prototype required resolving several significant integration hurdles. Below is a log of specific errors encountered and their resolutions.\n",
        "\n",
        "Error 1: Environment Constraints & Dependency Weight\n",
        "- Issue: \"ImportError: No module named 'vllm'\" and disk space exhaustion during installation.\n",
        "- Diagnosis: The environment could not support the heavy CUDA binaries required by the original requirements.txt.\n",
        "- Resolution: Wrote a custom bootstrapping script that physically creates dummy directories and python packages in the file system to satisfy the import check mechanisms of the 'transformers' library, which inspects package specs.\n",
        "\n",
        "Error 2: Hydra Configuration Schema Mismatch\n",
        "- Issue: \"omegaconf.errors.ConfigAttributeError: Key 'top_k' is not in struct\".\n",
        "- Diagnosis: The repository uses strict structured configurations (Data Classes). My API-based implementation required parameters ('top_k', 'use_openrouter') that were not defined in the original 'TeacherModelConfig' struct.\n",
        "- Resolution: Analyzed 'config/train_rl_model.py' and wrote a dynamic patch script to overwrite the Data Class definitions, explicitly adding the missing fields to the schema so the configuration manager (Hydra) would validate them.\n",
        "\n",
        "Error 3: Syntax Error in Dynamic Patching\n",
        "- Issue: \"AttributeError: 'TopologyConversation' object has no attribute 'getattr'\".\n",
        "- Diagnosis: A regex-based patch I wrote to make configuration access safer accidentally introduced invalid syntax, converting \"getattr(self.config...)\" into \"self.getattr(config...)\".\n",
        "- Resolution: Performed a surgical find-and-replace on 'src/classroom.py' to restore standard Python built-in function syntax.\n",
        "\n",
        "Error 4: The \"Cliff Edge\" Optimization Problem\n",
        "- Issue: Evolution results showed no improvement (Scores flatlined at 5.0).\n",
        "- Diagnosis: The fitness function had an overly strict penalty (-50) for any mention of the answer. Since instruction-tuned models are trained to be helpful, they constantly leaked answers, resulting in a fitness of 0 for almost the entire population. This removed the \"gradient\" needed for evolution.\n",
        "- Resolution:\n",
        "    1. Softened the leakage penalty to -15 to allow \"imperfect but promising\" strategies to survive and reproduce.\n",
        "    2. Increased the conversation horizon from 4 to 6 turns to give the strategy time to work.\n",
        "    3. Injected a \"SYSTEM: FORBIDDEN\" instruction into the phenotype wrapper to help the model adhere to constraints.\n",
        "\n",
        "Error 5: JSON Serialization Failure\n",
        "- Issue: \"TypeError: Object of type float64 is not JSON serializable\".\n",
        "- Diagnosis: The metrics tracking used NumPy for mean calculation, which returns numpy types that the standard Python JSON library cannot parse.\n",
        "- Resolution: Implemented a custom 'NumpyEncoder' class inheriting from 'json.JSONEncoder' to automatically convert NumPy data types to standard Python floats during the export process.\n",
        "\n",
        "-------------------------------------------------------------------------\n",
        "\n",
        "5. RESULTS AND CONCLUSION\n",
        "\n",
        "The final execution of the prototype demonstrated successful evolutionary learning:\n",
        "\n",
        "- Optimization: The population mean fitness improved from 73.3 (Generation 0) to 88.3 (Generation 5).\n",
        "- Behavioral Shift: The algorithm converged on a Socratic strategy sequence: [Diagnose -> Verify -> Encourage -> Diagnose]. This contrasts with the baseline random strategies that often attempted to lecture immediately.\n",
        "- Diversity: The final structural diversity score was 0.50, indicating that the population maintained variation and successfully avoided mode collapse.\n",
        "\n",
        "\n",
        "This prototype validates that separating pedagogical structure from text generation is a viable path toward more robust and efficient AI tutoring systems."
      ],
      "metadata": {
        "id": "mlpRlIX4p07P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/eth-lre/pedagogicalrl.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI-LReJbGWL4",
        "outputId": "c1cf4343-7a7f-4b97-d5a6-950cbbf17097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pedagogicalrl'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 114 (delta 41), reused 69 (delta 20), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (114/114), 672.30 KiB | 3.46 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/pedagogicalrl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncb-pPwAGtKg",
        "outputId": "a56b8f46-52e9-4c5c-ee06-666ca30b8404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/pedagogicalrl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install ONLY lightweight tools (No heavy torch/cuda downloads)\n",
        "!pip install hydra-core omegaconf python-dotenv openai google-generativeai colorama --quiet\n",
        "\n",
        "# This code creates a fake vLLM library in memory that matches the repo's imports exactly.\n",
        "import sys\n",
        "import types\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Any, Optional\n",
        "from importlib.machinery import ModuleSpec\n",
        "\n",
        "print(\"üõ°Ô∏è Initializing Robust vLLM Mocking...\")\n",
        "\n",
        "# Helper to create valid modules with specs (Satisfies 'transformers' checks)\n",
        "def mock_module(name):\n",
        "    m = types.ModuleType(name)\n",
        "    m.__spec__ = ModuleSpec(name=name, loader=None)\n",
        "    sys.modules[name] = m\n",
        "    return m\n",
        "\n",
        "# --- A. Mock 'vllm' Top-Level ---\n",
        "vllm = mock_module(\"vllm\")\n",
        "\n",
        "@dataclass\n",
        "class SamplingParams:\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 1.0\n",
        "    top_k: int = -1\n",
        "    max_tokens: int = 100\n",
        "    n: int = 1\n",
        "    logits_processors: Any = None\n",
        "    stop: Optional[List[str]] = None\n",
        "\n",
        "@dataclass\n",
        "class CompletionOutput:\n",
        "    index: int\n",
        "    text: str\n",
        "    token_ids: List[int]\n",
        "    cumulative_logprob: float\n",
        "    logprobs: List[Any]\n",
        "\n",
        "@dataclass\n",
        "class RequestOutput:\n",
        "    request_id: str\n",
        "    prompt: str\n",
        "    outputs: List[CompletionOutput]\n",
        "    prompt_token_ids: List[int]\n",
        "    prompt_logprobs: List[Any]\n",
        "    finished: bool\n",
        "\n",
        "class PoolingOutput:\n",
        "    pass\n",
        "\n",
        "class LLM:\n",
        "    def __init__(self, *args, **kwargs): pass\n",
        "    def encode(self, *args, **kwargs): return []\n",
        "    def chat(self, *args, **kwargs): return []\n",
        "\n",
        "vllm.SamplingParams = SamplingParams\n",
        "vllm.CompletionOutput = CompletionOutput\n",
        "vllm.RequestOutput = RequestOutput\n",
        "vllm.PoolingOutput = PoolingOutput\n",
        "vllm.LLM = LLM\n",
        "\n",
        "# --- B. Mock 'vllm.config' (for pedagogical_reward.py) ---\n",
        "vllm_config = mock_module(\"vllm.config\")\n",
        "\n",
        "class PoolerConfig:\n",
        "    def __init__(self, pooling_type, **kwargs): pass\n",
        "\n",
        "vllm_config.PoolerConfig = PoolerConfig\n",
        "vllm.config = vllm_config\n",
        "\n",
        "# --- C. Mock 'vllm.distributed.parallel_state' (for data_parallel_vllm.py) ---\n",
        "vllm_dist = mock_module(\"vllm.distributed\")\n",
        "vllm_dist_ps = mock_module(\"vllm.distributed.parallel_state\")\n",
        "\n",
        "def dummy_destroy(): pass\n",
        "vllm_dist_ps.destroy_model_parallel = dummy_destroy\n",
        "vllm_dist_ps.destroy_distributed_environment = dummy_destroy\n",
        "\n",
        "vllm_dist.parallel_state = vllm_dist_ps\n",
        "vllm.distributed = vllm_dist\n",
        "\n",
        "# --- D. Mock 'liger_kernel' & 'deepspeed' (for trainer imports) ---\n",
        "mock_module(\"liger_kernel\")\n",
        "mock_module(\"liger_kernel.chunked_loss\")\n",
        "mock_module(\"deepspeed\")\n",
        "\n",
        "print(\"‚úÖ vLLM, DeepSpeed, and Liger Kernel successfully mocked.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lGvcShZFKfc",
        "outputId": "718eef7a-c977-4b9c-ee26-1e1b5e60a852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ°Ô∏è Initializing Robust vLLM Mocking...\n",
            "‚úÖ vLLM, DeepSpeed, and Liger Kernel successfully mocked.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. TOPOLOGY LOGIC (Genotype)\n",
        "code_topology = \"\"\"\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "class Action:\n",
        "    DIAGNOSE = \"diagnose\"\n",
        "    SCAFFOLD = \"scaffold\"\n",
        "    HINT = \"hint\"\n",
        "    VERIFY = \"verify\"\n",
        "    ENCOURAGE = \"encourage\"\n",
        "\n",
        "    @staticmethod\n",
        "    def all(): return [Action.DIAGNOSE, Action.SCAFFOLD, Action.HINT, Action.VERIFY, Action.ENCOURAGE]\n",
        "\n",
        "@dataclass\n",
        "class Topology:\n",
        "    genes: List[str]\n",
        "    fitness: float = -999.0\n",
        "\n",
        "    def get_instruction(self, turn_idx: int) -> str:\n",
        "        if turn_idx >= len(self.genes): return \"Guide the student gently.\"\n",
        "        step = self.genes[turn_idx]\n",
        "        prompts = {\n",
        "            Action.DIAGNOSE: \"Do NOT solve. Ask the student what they think the first step is.\",\n",
        "            Action.SCAFFOLD: \"Break the problem down. Create a simpler example with different numbers.\",\n",
        "            Action.HINT: \"Give a conceptual hint about the formula, but do NOT mention the numbers.\",\n",
        "            Action.VERIFY: \"Ask the student to double-check their last calculation.\",\n",
        "            Action.ENCOURAGE: \"Tell them they are making progress, but ask them to try the step again.\"\n",
        "        }\n",
        "        return prompts.get(step, \"Guide the student.\")\n",
        "\n",
        "    def mutate(self):\n",
        "        idx = random.randint(0, len(self.genes)-1)\n",
        "        self.genes[idx] = random.choice(Action.all())\n",
        "\n",
        "    @classmethod\n",
        "    def crossover(cls, p1, p2):\n",
        "        if len(p1.genes) < 2: return cls(genes=p1.genes)\n",
        "        split = random.randint(1, len(p1.genes)-1)\n",
        "        return cls(genes=p1.genes[:split] + p2.genes[split:])\n",
        "\n",
        "    @classmethod\n",
        "    def random_init(cls, length=4):\n",
        "        return cls(genes=random.choices(Action.all(), k=length))\n",
        "\"\"\"\n",
        "with open(\"src/topology.py\", \"w\") as f: f.write(code_topology)\n",
        "\n",
        "# 2. CLASSROOM WRAPPER (Phenotype)\n",
        "code_top_class = \"\"\"\n",
        "from src.classroom import Conversation, ConversationState\n",
        "from src.topology import Topology\n",
        "from jinja2 import Template\n",
        "\n",
        "class TopologyConversation(Conversation):\n",
        "    def __init__(self, problem, answer, generation_cfg, topology: Topology):\n",
        "        super().__init__(problem, answer, generation_cfg)\n",
        "        self.topology = topology\n",
        "        self.turn_count = 0\n",
        "        self.template = Template(\n",
        "            \"TASK: Math Tutor.\\\\n\"\n",
        "            \"STRATEGY: {{instruction}}\\\\n\"\n",
        "            \"PROBLEM: {{problem}}\\\\n\"\n",
        "            \"HISTORY: {{history}}\\\\n\"\n",
        "            \"RESPONSE:\"\n",
        "        )\n",
        "\n",
        "    def get_conversation(self):\n",
        "        if self.state == ConversationState.TEACHER_TURN:\n",
        "            instruction = self.topology.get_instruction(self.turn_count)\n",
        "            hist = \"\\\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in self.conversation[-4:]])\n",
        "            prompt = self.template.render(problem=self.problem, instruction=instruction, history=hist)\n",
        "            self.turn_count += 1\n",
        "            return [{\"role\": \"user\", \"content\": prompt}]\n",
        "        return super().get_conversation()\n",
        "\"\"\"\n",
        "with open(\"src/topology_classroom.py\", \"w\") as f: f.write(code_top_class)\n",
        "\n",
        "# 3. EVOLUTION RUNNER (Main Script)\n",
        "code_runner = \"\"\"\n",
        "# RE-INJECT MOCKS FOR THE SCRIPT PROCESS\n",
        "import sys, types\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Any\n",
        "from importlib.machinery import ModuleSpec\n",
        "\n",
        "def mock_module(name):\n",
        "    m = types.ModuleType(name)\n",
        "    m.__spec__ = ModuleSpec(name=name, loader=None)\n",
        "    sys.modules[name] = m\n",
        "    return m\n",
        "\n",
        "if \"vllm\" not in sys.modules:\n",
        "    vllm = mock_module(\"vllm\")\n",
        "    @dataclass\n",
        "    class SamplingParams:\n",
        "        temperature: float = 0.7; top_p: float = 1.0; top_k: int = -1; max_tokens: int = 100; n: int = 1; logits_processors: Any = None; stop: Any = None\n",
        "    @dataclass\n",
        "    class CompletionOutput:\n",
        "        index: int; text: str; token_ids: List[int]; cumulative_logprob: float; logprobs: List[Any]\n",
        "    @dataclass\n",
        "    class RequestOutput:\n",
        "        request_id: str; prompt: str; outputs: List[CompletionOutput]; prompt_token_ids: List[int]; prompt_logprobs: List[Any]; finished: bool\n",
        "    class PoolingOutput: pass\n",
        "    class LLM:\n",
        "        def __init__(self, *args, **kwargs): pass\n",
        "        def encode(self, *args, **kwargs): return []\n",
        "        def chat(self, *args, **kwargs): return []\n",
        "\n",
        "    vllm.SamplingParams = SamplingParams; vllm.CompletionOutput = CompletionOutput\n",
        "    vllm.RequestOutput = RequestOutput; vllm.PoolingOutput = PoolingOutput; vllm.LLM = LLM\n",
        "\n",
        "    vllm_config = mock_module(\"vllm.config\")\n",
        "    class PoolerConfig:\n",
        "        def __init__(self, pooling_type, **kwargs): pass\n",
        "    vllm_config.PoolerConfig = PoolerConfig\n",
        "    vllm.config = vllm_config\n",
        "\n",
        "    vllm_dist = mock_module(\"vllm.distributed\")\n",
        "    vllm_dist_ps = mock_module(\"vllm.distributed.parallel_state\")\n",
        "    vllm_dist_ps.destroy_model_parallel = lambda: None\n",
        "    vllm_dist_ps.destroy_distributed_environment = lambda: None\n",
        "    vllm_dist.parallel_state = vllm_dist_ps\n",
        "    vllm.distributed = vllm_dist\n",
        "\n",
        "    mock_module(\"liger_kernel\")\n",
        "    mock_module(\"liger_kernel.chunked_loss\")\n",
        "    mock_module(\"deepspeed\")\n",
        "\n",
        "# --- ACTUAL SCRIPT STARTS HERE ---\n",
        "import hydra\n",
        "import random\n",
        "import copy\n",
        "import logging\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from src.classroom import Classroom, ConversationState\n",
        "from src.topology import Topology, Action\n",
        "from src.topology_classroom import TopologyConversation\n",
        "from config.eval import EvalConfig\n",
        "from hydra.core.config_store import ConfigStore\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
        "logger = logging.getLogger()\n",
        "\n",
        "cs = ConfigStore.instance()\n",
        "cs.store(name=\"config\", node=EvalConfig)\n",
        "\n",
        "def calculate_fitness(conv, ans, length):\n",
        "    hist = conv.conversation\n",
        "    st_msgs = [m['content'] for m in hist if m['role'] == 'student']\n",
        "    te_msgs = [m['content'] for m in hist if m['role'] == 'teacher']\n",
        "\n",
        "    if not st_msgs: return 0.0\n",
        "\n",
        "    success = str(ans) in st_msgs[-1]\n",
        "    score = 100.0 if success else 10.0\n",
        "    if success: score += (length - len(te_msgs)) * 15\n",
        "    for m in te_msgs:\n",
        "        if str(ans) in m: score -= 50.0\n",
        "    return score\n",
        "\n",
        "@hydra.main(config_path=\"config/eval\", version_base=None)\n",
        "def main(cfg: EvalConfig):\n",
        "    load_dotenv()\n",
        "    print(\"\\\\nüß¨ INITIALIZING EVOLUTIONARY TOPOLOGIES (SECURE MODE)...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    POP_SIZE = 4\n",
        "    GENERATIONS = 3\n",
        "    PROBLEM = \"Solve for x: 3x + 12 = 27\"\n",
        "    ANSWER = \"5\"\n",
        "\n",
        "    classroom = Classroom(cfg.student_model, cfg.teacher_model, cfg.judge_model, cfg.reward_model, cfg.generation, None)\n",
        "\n",
        "    pop = [Topology.random_init() for _ in range(POP_SIZE)]\n",
        "    best_score = -999\n",
        "\n",
        "    for gen in range(GENERATIONS):\n",
        "        print(f\"\\\\n‚ö° GENERATION {gen+1}/{GENERATIONS}\")\n",
        "        conversations = []\n",
        "        for dna in pop:\n",
        "            c = TopologyConversation(PROBLEM, ANSWER, cfg.generation, topology=dna)\n",
        "            c.start_conversation()\n",
        "            conversations.append(c)\n",
        "\n",
        "        for _ in range(4):\n",
        "            act_t = [c for c in conversations if c.state == ConversationState.TEACHER_TURN]\n",
        "            if act_t: classroom.generate_next_teacher_utterances(act_t)\n",
        "            act_s = [c for c in conversations if c.state == ConversationState.STUDENT_TURN]\n",
        "            if act_s: classroom.generate_next_student_utterances(act_s)\n",
        "\n",
        "        scores = []\n",
        "        for i, c in enumerate(conversations):\n",
        "            f = calculate_fitness(c, ANSWER, 4)\n",
        "            pop[i].fitness = f\n",
        "            scores.append(f)\n",
        "            status = \"‚úÖ Solved\" if f > 50 else \"‚ùå Failed\"\n",
        "            print(f\"   [Org {i}] {pop[i].genes} | Score: {f:.1f}\")\n",
        "\n",
        "        if max(scores) > best_score:\n",
        "            best_score = max(scores)\n",
        "\n",
        "        new_pop = []\n",
        "        while len(new_pop) < POP_SIZE:\n",
        "            p = random.choice(pop)\n",
        "            child = copy.deepcopy(p)\n",
        "            if random.random() < 0.6: child.mutate()\n",
        "            new_pop.append(child)\n",
        "        pop = new_pop\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*60)\n",
        "    print(\"üèÜ EVOLUTION COMPLETE\")\n",
        "    print(f\"Final Best Score: {best_score}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "with open(\"run_evolution.py\", \"w\") as f: f.write(code_runner)\n",
        "\n",
        "print(\"‚úÖ Research files created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TYaTU_0LZyM",
        "outputId": "e775b3df-c724-484f-99c2-6651453c78fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Research files created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "classroom_path = \"src/classroom.py\"\n",
        "\n",
        "print(f\"üîß Repairing {classroom_path}...\")\n",
        "\n",
        "with open(classroom_path, \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# FIX 1: Replace the broken \"self.getattr(generation_cfg\" with \"getattr(self.generation_cfg\"\n",
        "# This handles the specific error you saw in the traceback.\n",
        "if \"self.getattr(generation_cfg\" in content:\n",
        "    content = content.replace(\"self.getattr(generation_cfg\", \"getattr(self.generation_cfg\")\n",
        "    print(\"   -> Fixed 'self.getattr(generation_cfg' instances.\")\n",
        "\n",
        "# FIX 2: Catch cases where it might be \"self.getattr(self.generation_cfg\" (rare but possible artifact)\n",
        "if \"self.getattr(self.generation_cfg\" in content:\n",
        "    content = content.replace(\"self.getattr(self.generation_cfg\", \"getattr(self.generation_cfg\")\n",
        "    print(\"   -> Fixed 'self.getattr(self.generation_cfg' instances.\")\n",
        "\n",
        "# FIX 3: Ensure getattr is used correctly for 'max_turns' which might appear as self.getattr(generation_cfg...\n",
        "# We simply do a global replace for the pattern caused by the previous regex script.\n",
        "content = content.replace(\"self.getattr(\", \"getattr(self.\")\n",
        "\n",
        "# FIX 4: Re-verify local variables vs self attributes\n",
        "# If the previous script replaced 'generation_cfg.attr' (local var) with 'getattr(generation_cfg, ...)'\n",
        "# that is valid. We only want to remove 'self.' from BEFORE getattr.\n",
        "# The replace above (FIX 3) might have been too aggressive if 'self.getattr' wasn't followed by 'generation_cfg'.\n",
        "# Let's check specifically for the pattern in the traceback.\n",
        "\n",
        "# Reload content to be clean and do precise replacement\n",
        "with open(classroom_path, \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Precise Fixes based on your traceback\n",
        "# The bad pattern: self.getattr(generation_cfg, 'max_tokens_in_conversation', 8192)\n",
        "# The wanted pattern: getattr(self.generation_cfg, 'max_tokens_in_conversation', 8192)\n",
        "\n",
        "patched_content = content.replace(\"self.getattr(generation_cfg\", \"getattr(self.generation_cfg\")\n",
        "\n",
        "# Write back\n",
        "with open(classroom_path, \"w\") as f:\n",
        "    f.write(patched_content)\n",
        "    f.flush()\n",
        "    os.fsync(f.fileno())\n",
        "\n",
        "print(\"‚úÖ File repaired. The AttributeError should be resolved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7j58lcEG-yw",
        "outputId": "e97d6a2f-6724-4080-8ecf-73f4b0b6f067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Repairing src/classroom.py...\n",
            "‚úÖ File repaired. The AttributeError should be resolved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acc6baa1",
        "outputId": "af17faba-5a8b-4045-b83e-511a689aefed"
      },
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Load Secrets Securely\n",
        "try:\n",
        "    # Try OpenRouter first\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = userdata.get('OPENROUTER_API_KEY')\n",
        "    print(\"‚úÖ Loaded OPENROUTER_API_KEY\")\n",
        "except:\n",
        "    try:\n",
        "        # Try Gemini\n",
        "        os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "        print(\"‚úÖ Loaded GEMINI_API_KEY\")\n",
        "    except:\n",
        "        print(\"‚ùå ERROR: Keys not found. Please set secrets in Colab sidebar.\")\n",
        "\n",
        "# 2. Run\n",
        "!python run_evolution.py \\\n",
        "  --config-name Qwen2.5-7B-Instruct.yaml \\\n",
        "  teacher_model.use_openrouter=True \\\n",
        "  teacher_model.model_name_or_path=\"meta-llama/llama-3.1-8b-instruct\" \\\n",
        "  +student_model.use_openrouter=True \\\n",
        "  student_model.model_name_or_path=\"meta-llama/llama-3.1-8b-instruct\" \\\n",
        "  +judge_model.use_openrouter=True \\\n",
        "  judge_model.model_name_or_path=\"meta-llama/llama-3.1-8b-instruct\" \\\n",
        "  +generation.number_judge_attempts=0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded OPENROUTER_API_KEY\n",
            "\n",
            "üß¨ INITIALIZING EVOLUTIONARY TOPOLOGIES (SECURE MODE)...\n",
            "============================================================\n",
            "\n",
            "‚ö° GENERATION 1/3\n",
            "[2025-12-14 18:25:36,063][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:36,107][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:36,251][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:36,383][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:25:40,925][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:41,269][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:41,329][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:41,519][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:25:45,447][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:45,595][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:45,610][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:45,614][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:25:49,198][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:49,339][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:49,686][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:49,764][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:25:54,598][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:54,615][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:54,623][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:55,115][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:25:57,955][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:58,185][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:58,285][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:25:58,387][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:01,128][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:01,397][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:01,480][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:01,587][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "   [Org 0] ['diagnose', 'hint', 'scaffold', 'diagnose'] | Score: -40.0\n",
            "   [Org 1] ['encourage', 'hint', 'encourage', 'diagnose'] | Score: -90.0\n",
            "   [Org 2] ['encourage', 'hint', 'scaffold', 'diagnose'] | Score: 15.0\n",
            "   [Org 3] ['diagnose', 'diagnose', 'verify', 'diagnose'] | Score: 65.0\n",
            "\n",
            "‚ö° GENERATION 2/3\n",
            "[2025-12-14 18:26:03,171][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:03,510][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:03,593][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:03,644][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:06,398][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:06,475][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:06,696][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:06,791][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:07,936][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:08,266][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:08,273][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:08,377][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:10,479][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:10,631][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:10,689][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:10,727][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:15,183][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:15,573][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:15,749][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:16,041][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:17,211][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:17,442][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:17,641][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:17,708][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:23,346][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:23,461][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:23,641][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:23,646][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "   [Org 0] ['encourage', 'hint', 'scaffold', 'diagnose'] | Score: -35.0\n",
            "   [Org 1] ['diagnose', 'diagnose', 'scaffold', 'diagnose'] | Score: -40.0\n",
            "   [Org 2] ['encourage', 'hint', 'encourage', 'diagnose'] | Score: -90.0\n",
            "   [Org 3] ['diagnose', 'hint', 'scaffold', 'diagnose'] | Score: -90.0\n",
            "\n",
            "‚ö° GENERATION 3/3\n",
            "[2025-12-14 18:26:26,083][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:26,116][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:26,324][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:26,387][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:32,014][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:32,220][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:32,318][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:32,347][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:37,174][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:37,227][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:37,597][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:37,629][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:42,825][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:43,091][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:43,316][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:43,324][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:45,324][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:45,486][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:45,529][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:45,713][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:48,485][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:48,719][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:48,775][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:48,786][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "[2025-12-14 18:26:52,756][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:52,803][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:52,909][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2025-12-14 18:26:53,060][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "Attempt 1 succeeded\n",
            "   [Org 0] ['diagnose', 'scaffold', 'scaffold', 'diagnose'] | Score: -90.0\n",
            "   [Org 1] ['verify', 'hint', 'encourage', 'diagnose'] | Score: -40.0\n",
            "   [Org 2] ['diagnose', 'hint', 'scaffold', 'diagnose'] | Score: 65.0\n",
            "   [Org 3] ['diagnose', 'diagnose', 'scaffold', 'diagnose'] | Score: 115.0\n",
            "\n",
            "============================================================\n",
            "üèÜ EVOLUTION COMPLETE\n",
            "Final Best Score: 115.0\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NEW IMPROVED VERSION**"
      ],
      "metadata": {
        "id": "4XH390NoBd2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the root directory\n",
        "base_dir = \"/content/drive/MyDrive/pedagogicalrl\"\n",
        "os.chdir(base_dir)\n",
        "\n",
        "print(\" Creating physical mocks for heavy libraries...\")\n",
        "\n",
        "# --- 1. MOCK vLLM ---\n",
        "os.makedirs(\"vllm/config\", exist_ok=True)\n",
        "os.makedirs(\"vllm/distributed\", exist_ok=True)\n",
        "\n",
        "# vllm/__init__.py\n",
        "with open(\"vllm/__init__.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Any, Optional\n",
        "\n",
        "@dataclass\n",
        "class SamplingParams:\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 1.0\n",
        "    top_k: int = -1\n",
        "    max_tokens: int = 100\n",
        "    n: int = 1\n",
        "    logits_processors: Any = None\n",
        "    stop: Optional[List[str]] = None\n",
        "\n",
        "@dataclass\n",
        "class CompletionOutput:\n",
        "    index: int\n",
        "    text: str\n",
        "    token_ids: List[int]\n",
        "    cumulative_logprob: float\n",
        "    logprobs: List[Any]\n",
        "\n",
        "@dataclass\n",
        "class RequestOutput:\n",
        "    request_id: str\n",
        "    prompt: str\n",
        "    outputs: List[CompletionOutput]\n",
        "    prompt_token_ids: List[int]\n",
        "    prompt_logprobs: List[Any]\n",
        "    finished: bool\n",
        "\n",
        "class PoolingOutput:\n",
        "    pass\n",
        "\n",
        "class LLM:\n",
        "    def __init__(self, *args, **kwargs): pass\n",
        "    def encode(self, *args, **kwargs): return []\n",
        "    def chat(self, *args, **kwargs): return []\n",
        "\"\"\")\n",
        "\n",
        "# vllm/config.py\n",
        "with open(\"vllm/config.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "class PoolerConfig:\n",
        "    def __init__(self, pooling_type, **kwargs): pass\n",
        "\"\"\")\n",
        "\n",
        "# vllm/distributed/parallel_state.py\n",
        "with open(\"vllm/distributed/parallel_state.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "def destroy_model_parallel(): pass\n",
        "def destroy_distributed_environment(): pass\n",
        "\"\"\")\n",
        "\n",
        "# vllm/distributed/__init__.py\n",
        "with open(\"vllm/distributed/__init__.py\", \"w\") as f:\n",
        "    f.write(\"from .parallel_state import *\")\n",
        "\n",
        "\n",
        "# --- 2. MOCK LIGER KERNEL ---\n",
        "os.makedirs(\"liger_kernel\", exist_ok=True)\n",
        "with open(\"liger_kernel/__init__.py\", \"w\") as f:\n",
        "    f.write(\"from . import chunked_loss\")\n",
        "\n",
        "with open(\"liger_kernel/chunked_loss.py\", \"w\") as f:\n",
        "    f.write(\"class LigerFusedLinearGRPOLoss: pass\")\n",
        "\n",
        "\n",
        "# --- 3. MOCK DEEPSPEED ---\n",
        "os.makedirs(\"deepspeed\", exist_ok=True)\n",
        "with open(\"deepspeed/__init__.py\", \"w\") as f:\n",
        "    f.write(\"pass\")\n",
        "\n",
        "\n",
        "# --- 4. MOCK PYNVML (GPU Monitor) ---\n",
        "os.makedirs(\"pynvml\", exist_ok=True)\n",
        "with open(\"pynvml/__init__.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "def nvmlInit(): pass\n",
        "def nvmlDeviceGetHandleByIndex(i): return None\n",
        "class MockMem:\n",
        "    used = 0\n",
        "    total = 1\n",
        "def nvmlDeviceGetMemoryInfo(h): return MockMem()\n",
        "\"\"\")\n",
        "\n",
        "print(\" Physical mocks created in file system. Import errors are resolved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j43jeOTsMUPp",
        "outputId": "daa07348-1756-4230-98fd-39d42c002eca"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Creating physical mocks for heavy libraries...\n",
            " Physical mocks created in file system. Import errors are resolved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch config/train_rl_model.py (Fix Hydra Structs)\n",
        "config_content = \"\"\"\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "@dataclass\n",
        "class LoraConfig:\n",
        "    enable: bool = False\n",
        "    rank: int = 16\n",
        "    alpha: float = 32\n",
        "    target_modules: Any = \"all-linear\"\n",
        "    dropout: float = 0.01\n",
        "    bias: str = \"none\"\n",
        "\n",
        "@dataclass\n",
        "class ModelvLLMConfig:\n",
        "    temperature: float = 0.9\n",
        "    top_k: int = 50\n",
        "    top_p: float = 1.0\n",
        "    max_length: int = 8192\n",
        "    max_num_seqs: int = 256\n",
        "    gpu_memory_utilization: float = 0.5\n",
        "    number_of_gpus_per_instance: int = 4\n",
        "    max_number_of_instances: int = -1\n",
        "    from_0: bool = True\n",
        "    load_and_unload: bool = True\n",
        "    bits_and_bytes: bool = False\n",
        "    enable_sleep_mode: bool = True\n",
        "    use_v0: bool = True\n",
        "    enforce_eager: bool = False\n",
        "\n",
        "@dataclass\n",
        "class TeacherModelConfig:\n",
        "    model_name_or_path: str = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "    use_openrouter: bool = False\n",
        "    use_gemini: bool = False\n",
        "    vllm: ModelvLLMConfig = field(default_factory=ModelvLLMConfig)\n",
        "    lora: LoraConfig = field(default_factory=LoraConfig)\n",
        "\n",
        "@dataclass\n",
        "class StudentModelConfig:\n",
        "    model_name_or_path: str = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\"\n",
        "    use_openrouter: bool = False\n",
        "    use_gemini: bool = False\n",
        "    vllm: ModelvLLMConfig = field(default_factory=ModelvLLMConfig)\n",
        "\n",
        "@dataclass\n",
        "class JudgeModelConfig:\n",
        "    model_name_or_path: str = \"Qwen/Qwen2.5-14B-Instruct-AWQ\"\n",
        "    use_openrouter: bool = False\n",
        "    use_gemini: bool = False\n",
        "    vllm: ModelvLLMConfig = field(default_factory=ModelvLLMConfig)\n",
        "\n",
        "@dataclass\n",
        "class RewardModelConfig:\n",
        "    model_name_or_path: str = \"Qwen/Qwen2.5-Math-RM-72B\"\n",
        "    vllm: ModelvLLMConfig = field(default_factory=ModelvLLMConfig)\n",
        "\n",
        "@dataclass\n",
        "class GenerationConfig:\n",
        "    student_personas_prompts_paths: Dict[str, str] = field(default_factory=lambda: {\"simple_student\": \"prompt_templates/personas/simple_student.txt\"})\n",
        "    judges_rules_prompts_paths: Dict[str, str] = field(default_factory=lambda: {\"does_not_leak_answer\": \"prompt_templates/judges/does_not_leak_answer.txt\", \"follows_pedagogical_values\": \"prompt_templates/judges/follows_pedagogical_values.txt\"})\n",
        "    student_initial_attempt_prompt_path: str = \"prompt_templates/student_initial_attempt_prompt.txt\"\n",
        "    student_final_prompt_path: str = \"prompt_templates/student_final_prompt.txt\"\n",
        "    teacher_prompt_path: str = \"prompt_templates/teacher_prompt.txt\"\n",
        "    initial_attempt_wrapper_prompt_path: str = \"prompt_templates/initial_attempt_wrapper_prompt.txt\"\n",
        "    student_attempt_prompt_path: str = \"prompt_templates/student_attempt_prompt.txt\"\n",
        "    max_turns: int = 15\n",
        "    max_tokens_in_conversation: int = 8192\n",
        "    max_tokens_per_turn: int = 1024\n",
        "    max_tokens_per_student_attempt: int = 3900\n",
        "    max_tokens_per_judge_attempt: int = 2048\n",
        "    tokenizer_to_use: str = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "    number_student_attempts: int = 8\n",
        "    number_judge_attempts: int = 2\n",
        "    ignore_rejected_judge: bool = False\n",
        "    forced_conversation_type: Optional[str] = None\n",
        "    use_thinking: bool = False\n",
        "    force_thinking: bool = False\n",
        "    extra_penalty_for_rejected_judges: float = 0.25\n",
        "    server_port: int = 8005\n",
        "    use_experimental_shared_memory: bool = False\n",
        "    student_names: list[str | None] = field(default_factory=lambda: [\"Alex\", None])\n",
        "\n",
        "@dataclass\n",
        "class Dataset:\n",
        "    name_or_path: str = \"rd211/Big-Math-RL-Verified-Filtered\"\n",
        "    split: str = \"train\"\n",
        "    ratio: float = 1.0\n",
        "\n",
        "@dataclass\n",
        "class DatasetConfig:\n",
        "    train_datasets: list[Dataset] = field(default_factory=lambda: [Dataset()])\n",
        "    max_train_examples: int = -1\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    gradient_checkpointing: bool = True\n",
        "    num_samples_per_problem: int = 8\n",
        "    number_of_problems_per_batch: int = 16\n",
        "    per_device_train_batch_size: int = 2\n",
        "    lr_scheduler_type: str = \"constant\"\n",
        "    optimizer: str = \"paged_adamw_8bit\"\n",
        "    epochs: int = 1\n",
        "    max_steps: int = -1\n",
        "    deepspeed_config_path: Optional[str] = None\n",
        "    beta: float = 0.001\n",
        "    learning_rate: float = 5e-7\n",
        "    mu: int = 2\n",
        "    epsilon: float = 0.2\n",
        "    batch_size_ref_model: int = 4\n",
        "    save_policy_to_disk_every_n: int = 1\n",
        "\n",
        "@dataclass\n",
        "class HuggingFaceConfig:\n",
        "    name: str = \"<model_name>\"\n",
        "    push_to_hub: bool = False\n",
        "\n",
        "@dataclass\n",
        "class LoggingConfig:\n",
        "    wandb: bool = False\n",
        "    wandb_project: str = \"train_rl\"\n",
        "    wandb_run_name: str = \"Qwen2.5-7B-Instruct\"\n",
        "    wandb_entity: Optional[str] = None\n",
        "    run_group: str = \"7b\"\n",
        "    wandb_tags: list[str] = field(default_factory=list)\n",
        "    save_dir: str = \"checkpoints\"\n",
        "    save_steps: int = 10\n",
        "\n",
        "@dataclass\n",
        "class RLModelTrainingConfig:\n",
        "    train: TrainConfig = field(default_factory=TrainConfig)\n",
        "    teacher_model: TeacherModelConfig = field(default_factory=TeacherModelConfig)\n",
        "    student_model: StudentModelConfig = field(default_factory=StudentModelConfig)\n",
        "    judge_model: JudgeModelConfig = field(default_factory=JudgeModelConfig)\n",
        "    reward_model: RewardModelConfig = field(default_factory=RewardModelConfig)\n",
        "    dataset: DatasetConfig = field(default_factory=DatasetConfig)\n",
        "    huggingface: HuggingFaceConfig = field(default_factory=HuggingFaceConfig)\n",
        "    logging: LoggingConfig = field(default_factory=LoggingConfig)\n",
        "    generation: GenerationConfig = field(default_factory=GenerationConfig)\n",
        "    skip_first_samples: int = 0\n",
        "    seed: int = 42\n",
        "\"\"\"\n",
        "with open(\"config/train_rl_model.py\", \"w\") as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "# Patch src/classroom.py (Fix getattr usage)\n",
        "with open(\"src/classroom.py\", \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Fix self.getattr\n",
        "content = content.replace(\"self.getattr(generation_cfg\", \"getattr(self.generation_cfg\")\n",
        "content = content.replace(\"self.getattr(self.generation_cfg\", \"getattr(self.generation_cfg\")\n",
        "content = content.replace(\"self.getattr(\", \"getattr(self.\")\n",
        "\n",
        "# Fix specific top_k crash lines\n",
        "if \"top_k=student_model_cfg.vllm.top_k\" in content:\n",
        "    content = content.replace(\"top_k=student_model_cfg.vllm.top_k\", \"top_k=getattr(student_model_cfg.vllm, 'top_k', 50)\")\n",
        "if \"top_k=teacher_model_cfg.vllm.top_k\" in content:\n",
        "    content = content.replace(\"top_k=teacher_model_cfg.vllm.top_k\", \"top_k=getattr(teacher_model_cfg.vllm, 'top_k', 50)\")\n",
        "if \"top_k=judge_model_cfg.vllm.top_k\" in content:\n",
        "    content = content.replace(\"top_k=judge_model_cfg.vllm.top_k\", \"top_k=getattr(judge_model_cfg.vllm, 'top_k', 50)\")\n",
        "\n",
        "with open(\"src/classroom.py\", \"w\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\" Patched Config and Classroom files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZa1jcT2MaM2",
        "outputId": "a5936122-3343-45a8-9afb-8a650102feb4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Patched Config and Classroom files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Genotype\n",
        "code_topology = \"\"\"\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "class Action:\n",
        "    DIAGNOSE = \"diagnose\"\n",
        "    SCAFFOLD = \"scaffold\"\n",
        "    HINT = \"hint\"\n",
        "    VERIFY = \"verify\"\n",
        "    ENCOURAGE = \"encourage\"\n",
        "\n",
        "    @staticmethod\n",
        "    def all(): return [Action.DIAGNOSE, Action.SCAFFOLD, Action.HINT, Action.VERIFY, Action.ENCOURAGE]\n",
        "\n",
        "@dataclass\n",
        "class Topology:\n",
        "    genes: List[str]\n",
        "    fitness: float = -999.0\n",
        "\n",
        "    def get_instruction(self, turn_idx: int) -> str:\n",
        "        if turn_idx >= len(self.genes): return \"Guide the student gently.\"\n",
        "        step = self.genes[turn_idx]\n",
        "        prompts = {\n",
        "            Action.DIAGNOSE: \"Do NOT explain. Ask the student what they think the first step is.\",\n",
        "            Action.SCAFFOLD: \"Break the problem down. Give a similar example with different numbers.\",\n",
        "            Action.HINT: \"Give a conceptual hint about the operation needed, but do NOT say the number.\",\n",
        "            Action.VERIFY: \"Ask the student to verify their arithmetic.\",\n",
        "            Action.ENCOURAGE: \"Validate their effort and ask them to try the next step.\"\n",
        "        }\n",
        "        return prompts.get(step, \"Guide the student.\")\n",
        "\n",
        "    def mutate(self):\n",
        "        idx = random.randint(0, len(self.genes)-1)\n",
        "        self.genes[idx] = random.choice(Action.all())\n",
        "\n",
        "    @classmethod\n",
        "    def crossover(cls, p1, p2):\n",
        "        if len(p1.genes) < 2: return cls(genes=p1.genes)\n",
        "        split = random.randint(1, len(p1.genes)-1)\n",
        "        return cls(genes=p1.genes[:split] + p2.genes[split:])\n",
        "\n",
        "    @classmethod\n",
        "    def random_init(cls, length=4):\n",
        "        return cls(genes=random.choices(Action.all(), k=length))\n",
        "\"\"\"\n",
        "with open(\"src/topology.py\", \"w\") as f: f.write(code_topology)\n",
        "\n",
        "# Create Phenotype Wrapper\n",
        "import os\n",
        "\n",
        "# Overwrite src/topology_classroom.py with the NON-LEAKING version\n",
        "code_top_class = \"\"\"\n",
        "from src.classroom import Conversation, ConversationState\n",
        "from src.topology import Topology\n",
        "from jinja2 import Template\n",
        "\n",
        "class TopologyConversation(Conversation):\n",
        "    def __init__(self, problem, answer, generation_cfg, topology: Topology):\n",
        "        super().__init__(problem, answer, generation_cfg)\n",
        "        self.topology = topology\n",
        "        self.turn_count = 0\n",
        "\n",
        "\n",
        "        self.template = Template(\n",
        "            \"SYSTEM: You are a Socratic Math Tutor. \\\\n\"\n",
        "            \"RULE: Do NOT reveal the final answer. Guide the student to discover it.\\\\n\"\n",
        "            \"STRATEGY: {{instruction}}\\\\n\"\n",
        "            \"PROBLEM: {{problem}}\\\\n\"\n",
        "            \"HISTORY: {{history}}\\\\n\"\n",
        "            \"RESPONSE:\"\n",
        "        )\n",
        "\n",
        "    def get_conversation(self):\n",
        "        if self.state == ConversationState.TEACHER_TURN:\n",
        "            instruction = self.topology.get_instruction(self.turn_count)\n",
        "            hist = \"\\\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in self.conversation[-6:]])\n",
        "\n",
        "\n",
        "            prompt = self.template.render(\n",
        "                problem=self.problem,\n",
        "                instruction=instruction,\n",
        "                history=hist\n",
        "            )\n",
        "\n",
        "            self.turn_count += 1\n",
        "            return [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        return super().get_conversation()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"src/topology_classroom.py\", \"w\") as f:\n",
        "    f.write(code_top_class)\n",
        "\n",
        "print(\" src/topology_classroom.py fixed: Answer removed from Teacher Prompt.\")\n",
        "\n",
        "# Create Main Script\n",
        "code_runner = \"\"\"\n",
        "import hydra\n",
        "import random\n",
        "import copy\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from src.classroom import Classroom, ConversationState\n",
        "from src.topology import Topology, Action\n",
        "from src.topology_classroom import TopologyConversation\n",
        "from config.eval import EvalConfig\n",
        "from hydra.core.config_store import ConfigStore\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
        "logger = logging.getLogger()\n",
        "\n",
        "cs = ConfigStore.instance()\n",
        "cs.store(name=\"config\", node=EvalConfig)\n",
        "\n",
        "def measure_structural_diversity(population):\n",
        "    if not population: return 0.0\n",
        "    unique = len(set(tuple(t.genes) for t in population))\n",
        "    return unique / len(population)\n",
        "\n",
        "def fitness_proportional_selection(population, k=1):\n",
        "    if len(population) == 0: return []\n",
        "    fitnesses = [p.fitness for p in population]\n",
        "    min_fit = min(fitnesses)\n",
        "    adjusted = [f - min_fit + 1.0 for f in fitnesses]\n",
        "    total = sum(adjusted)\n",
        "    if total < 0.01: return random.sample(population, min(k, len(population)))\n",
        "    weights = [a / total for a in adjusted]\n",
        "    return random.choices(population, weights=weights, k=k)\n",
        "\n",
        "def calculate_single_fitness(conv, ans, length):\n",
        "    hist = conv.conversation\n",
        "    st_msgs = [m['content'] for m in hist if m['role'] == 'student']\n",
        "    te_msgs = [m['content'] for m in hist if m['role'] == 'teacher']\n",
        "\n",
        "    if not st_msgs: return 0.0\n",
        "\n",
        "    success = str(ans) in st_msgs[-1]\n",
        "\n",
        "    if success:\n",
        "        score = 100.0\n",
        "        turns_used = len(te_msgs)\n",
        "        if turns_used <= 2: score += 30\n",
        "        elif turns_used <= 4: score += 15\n",
        "    else:\n",
        "        score = 20.0\n",
        "        if len(st_msgs) > 2: score += 10\n",
        "\n",
        "    # Softened penalty to allow learning (was 40, now 15)\n",
        "    leakage_count = sum(1 for m in te_msgs if str(ans) in m)\n",
        "    score -= leakage_count * 15\n",
        "\n",
        "    return max(0.0, score)\n",
        "\n",
        "def evaluate_topology(topology, problems, classroom, gen_config):\n",
        "    scores = []\n",
        "    # Evaluate on all problems for robustness\n",
        "    for prob in problems:\n",
        "        conv = TopologyConversation(prob[\"problem\"], prob[\"answer\"], gen_config, topology=topology)\n",
        "        conv.start_conversation()\n",
        "\n",
        "        # 5 Turns allowed\n",
        "        for _ in range(5):\n",
        "            if conv.state == ConversationState.TEACHER_TURN:\n",
        "                classroom.generate_next_teacher_utterances([conv])\n",
        "            elif conv.state == ConversationState.STUDENT_TURN:\n",
        "                classroom.generate_next_student_utterances([conv])\n",
        "\n",
        "        s = calculate_single_fitness(conv, prob[\"answer\"], 5)\n",
        "        scores.append(s)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "@hydra.main(config_path=\"config/eval\", version_base=None)\n",
        "def main(cfg: EvalConfig):\n",
        "    load_dotenv()\n",
        "    print(\"\\\\n EVOLUTIONARY TOPOLOGY SEARCH \")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    POP_SIZE = 4\n",
        "    GENERATIONS = 4\n",
        "\n",
        "    PROBLEMS = [\n",
        "        {\"problem\": \"Solve for x: 3x + 12 = 27\", \"answer\": \"5\"},\n",
        "        {\"problem\": \"Solve for y: 2y - 8 = 10\", \"answer\": \"9\"},\n",
        "        {\"problem\": \"Solve for z: 5z + 3 = 18\", \"answer\": \"3\"},\n",
        "    ]\n",
        "\n",
        "    classroom = Classroom(cfg.student_model, cfg.teacher_model, cfg.judge_model, cfg.reward_model, cfg.generation, None)\n",
        "    pop = [Topology.random_init() for _ in range(POP_SIZE)]\n",
        "\n",
        "    history = {\"best\": [], \"avg\": [], \"diversity\": []}\n",
        "\n",
        "    for gen in range(GENERATIONS):\n",
        "        print(f\"\\\\n  GENERATION {gen+1}/{GENERATIONS}\")\n",
        "\n",
        "        for i, org in enumerate(pop):\n",
        "            org.fitness = evaluate_topology(org, PROBLEMS, classroom, cfg.generation)\n",
        "            status = \"‚úÖ\" if org.fitness > 60 else \"‚ùå\"\n",
        "            print(f\"   [Org {i}] {org.genes} | Score: {org.fitness:.1f} {status}\")\n",
        "\n",
        "        best = max(pop, key=lambda p: p.fitness)\n",
        "        avg = sum(p.fitness for p in pop) / len(pop)\n",
        "        div = measure_structural_diversity(pop)\n",
        "\n",
        "        history[\"best\"].append(best.fitness)\n",
        "        history[\"avg\"].append(avg)\n",
        "        history[\"diversity\"].append(div)\n",
        "\n",
        "        print(f\"   ‚Üí Best={best.fitness:.1f}, Avg={avg:.1f}, Div={div:.2f}\")\n",
        "\n",
        "        elite = copy.deepcopy(best)\n",
        "        new_pop = [elite]\n",
        "\n",
        "        while len(new_pop) < POP_SIZE:\n",
        "            if random.random() < 0.6:\n",
        "                p1 = fitness_proportional_selection(pop, k=1)[0]\n",
        "                child = copy.deepcopy(p1)\n",
        "                child.mutate()\n",
        "            else:\n",
        "                p1, p2 = fitness_proportional_selection(pop, k=2)\n",
        "                child = Topology.crossover(p1, p2)\n",
        "            new_pop.append(child)\n",
        "        pop = new_pop\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*60)\n",
        "    print(\" FINAL RESULTS \")\n",
        "    print(f\"Start Best: {history['best'][0]:.1f} -> End Best: {history['best'][-1]:.1f}\")\n",
        "    print(f\"Diversity: {history['diversity'][-1]:.2f}\")\n",
        "    print(f\"Best Strategy: {best.genes}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "with open(\"run_evolution.py\", \"w\") as f: f.write(code_runner)\n",
        "print(\" run_evolution.py ready.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ0kvqVWBuDp",
        "outputId": "24c41732-abff-4715-c471-5016fcac071f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " src/topology_classroom.py fixed: Answer removed from Teacher Prompt.\n",
            " run_evolution.py ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cfe77c3-e7c6-4598-a828-5917b6027071",
        "id": "LAop9yL9CX_u"
      },
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = userdata.get('OPENROUTER_API_KEY')\n",
        "    print(\" Loaded OPENROUTER_API_KEY\")\n",
        "except:\n",
        "    try:\n",
        "        os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "        print(\" Loaded GEMINI_API_KEY\")\n",
        "    except:\n",
        "        print(\" ERROR: Keys not found. Please set secrets in Colab sidebar.\")\n",
        "\n",
        "# 2. Run\n",
        "!python run_evolution.py \\\n",
        "  --config-name Qwen2.5-7B-Instruct.yaml \\\n",
        "  teacher_model.use_openrouter=True \\\n",
        "  teacher_model.model_name_or_path=\"meta-llama/llama-3.1-8b-instruct\" \\\n",
        "  +student_model.use_openrouter=True \\\n",
        "  student_model.model_name_or_path=\"meta-llama/llama-3.1-8b-instruct\" \\\n",
        "  +judge_model.use_openrouter=True \\\n",
        "  judge_model.model_name_or_path=\"meta-llama/llama-3.1-8b-instruct\" \\\n",
        "  +generation.number_judge_attempts=0"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loaded OPENROUTER_API_KEY\n",
            "\n",
            " EVOLUTIONARY TOPOLOGY SEARCH \n",
            "============================================================\n",
            "\n",
            "  GENERATION 1/4\n",
            "[2025-12-15 15:15:52,482][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:15:53,800][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:15:55,362][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:15:57,014][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:15:58,482][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:01,059][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:03,171][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:05,630][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:07,369][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:09,513][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:11,589][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:13,254][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:15,334][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:17,375][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:21,487][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 0] ['diagnose', 'encourage', 'encourage', 'scaffold'] | Score: 43.3 ‚ùå\n",
            "[2025-12-15 15:16:23,997][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:26,185][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:28,123][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:31,033][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:32,055][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:34,426][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:36,090][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:36,787][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:39,836][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:44,278][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:45,274][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:46,668][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:56,666][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:16:58,685][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:00,370][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 1] ['encourage', 'verify', 'verify', 'diagnose'] | Score: 81.7 ‚úÖ\n",
            "[2025-12-15 15:17:02,570][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:04,153][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:05,190][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:06,820][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:07,730][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:16,593][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:17,411][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:19,318][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:23,138][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:24,046][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:26,793][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:28,782][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:32,354][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:34,339][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:42,565][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 2] ['encourage', 'hint', 'encourage', 'verify'] | Score: 45.0 ‚ùå\n",
            "[2025-12-15 15:17:46,561][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:48,802][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:50,097][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:52,547][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:53,453][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:17:59,845][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:02,969][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:08,059][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:09,769][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:12,620][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:13,511][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:18,229][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:26,714][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:28,477][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:30,715][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 3] ['verify', 'hint', 'diagnose', 'diagnose'] | Score: 50.0 ‚ùå\n",
            "   ‚Üí Best=81.7, Avg=55.0, Div=1.00\n",
            "\n",
            "  GENERATION 2/4\n",
            "[2025-12-15 15:18:33,076][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:35,040][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:36,481][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:39,374][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:40,387][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:41,932][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:43,861][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:45,738][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:46,930][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:49,458][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:50,230][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:51,977][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:54,753][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:56,506][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:18:59,401][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 0] ['encourage', 'verify', 'verify', 'diagnose'] | Score: 83.3 ‚úÖ\n",
            "[2025-12-15 15:19:01,432][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:02,697][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:03,785][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:06,357][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:07,645][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:10,445][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:12,840][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:15,334][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:17,303][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:20,464][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:23,195][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:24,211][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:25,737][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:27,680][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:30,934][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 1] ['verify', 'hint', 'diagnose', 'diagnose'] | Score: 45.0 ‚ùå\n",
            "[2025-12-15 15:19:32,312][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:33,654][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:34,544][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:35,712][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:39,041][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:42,995][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:45,531][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:48,959][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:50,713][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:52,348][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:56,019][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:19:57,040][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:01,932][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:03,321][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:06,910][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 2] ['encourage', 'verify', 'diagnose', 'diagnose'] | Score: 71.7 ‚úÖ\n",
            "[2025-12-15 15:20:09,302][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:11,089][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:12,675][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:13,920][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:17,602][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:20,082][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:21,279][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:22,582][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:24,565][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:27,346][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:28,996][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:31,116][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:34,359][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:36,291][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:40,152][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 3] ['verify', 'hint', 'verify', 'diagnose'] | Score: 45.0 ‚ùå\n",
            "   ‚Üí Best=83.3, Avg=61.2, Div=1.00\n",
            "\n",
            "  GENERATION 3/4\n",
            "[2025-12-15 15:20:43,464][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:45,274][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:45,979][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:50,702][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:52,809][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:54,595][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:56,056][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:20:59,461][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:00,980][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:20,662][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:24,796][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:27,284][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:30,160][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:31,650][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:38,511][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 0] ['encourage', 'verify', 'verify', 'diagnose'] | Score: 78.3 ‚úÖ\n",
            "[2025-12-15 15:21:41,031][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:41,920][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:44,357][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:45,627][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:47,559][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:49,917][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:50,792][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:52,179][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:21:57,400][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:01,066][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:02,459][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:04,574][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:07,847][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:08,479][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:11,150][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 1] ['encourage', 'verify', 'diagnose', 'diagnose'] | Score: 105.0 ‚úÖ\n",
            "[2025-12-15 15:22:13,914][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:16,100][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:17,527][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:22,304][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:23,986][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:26,945][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:29,052][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:32,551][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:35,349][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:40,145][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:41,136][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:42,570][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:45,033][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:46,766][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:50,586][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 2] ['encourage', 'verify', 'verify', 'diagnose'] | Score: 83.3 ‚úÖ\n",
            "[2025-12-15 15:22:52,441][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:54,143][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:56,897][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:22:59,636][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:00,977][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:02,859][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:05,659][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:07,874][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:13,750][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:16,960][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:20,124][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:21,091][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:28,222][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:35,576][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:36,639][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 3] ['scaffold', 'verify', 'verify', 'diagnose'] | Score: 48.3 ‚ùå\n",
            "   ‚Üí Best=105.0, Avg=78.7, Div=0.75\n",
            "\n",
            "  GENERATION 4/4\n",
            "[2025-12-15 15:23:44,527][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:50,106][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:51,902][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:54,186][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:55,944][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:57,120][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:58,686][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:23:59,575][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:01,092][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:03,115][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:04,075][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:06,573][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:11,039][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:13,581][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:16,193][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 0] ['encourage', 'verify', 'diagnose', 'diagnose'] | Score: 11.7 ‚ùå\n",
            "[2025-12-15 15:24:19,602][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:21,691][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:23,075][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:25,662][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:27,406][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:29,520][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:31,439][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:44,111][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:47,072][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:49,703][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:52,312][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:53,858][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:57,229][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:24:59,230][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:03,449][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 1] ['encourage', 'verify', 'verify', 'diagnose'] | Score: 105.0 ‚úÖ\n",
            "[2025-12-15 15:25:05,226][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:07,719][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:10,153][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:12,604][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:13,382][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:18,063][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:20,557][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:23,054][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:24,100][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:26,234][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:28,500][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:30,270][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:31,748][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:33,590][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:36,117][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 2] ['encourage', 'verify', 'diagnose', 'diagnose'] | Score: 83.3 ‚úÖ\n",
            "[2025-12-15 15:25:37,642][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:39,652][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:44,506][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:49,163][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:50,916][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:53,720][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:55,847][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:25:58,960][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:26:01,306][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:26:05,386][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:26:06,070][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:26:08,237][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:26:10,807][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:26:18,055][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "[2025-12-15 15:26:19,389][httpx][INFO] - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Attempt 1 succeeded\n",
            "   [Org 3] ['encourage', 'verify', 'verify', 'verify'] | Score: 120.0 ‚úÖ\n",
            "   ‚Üí Best=120.0, Avg=80.0, Div=0.75\n",
            "\n",
            "============================================================\n",
            " FINAL RESULTS \n",
            "Start Best: 81.7 -> End Best: 120.0\n",
            "Diversity: 0.75\n",
            "Best Strategy: ['encourage', 'verify', 'verify', 'verify']\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ohyDgwBPord"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}