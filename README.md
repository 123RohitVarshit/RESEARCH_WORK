# Evolutionary Pedagogical Topologies (EPT)

A research framework for evolving optimal AI teaching strategies using genetic algorithms. EPT separates the **reasoning structure** (Genotype) from the **text generation** (Phenotype) to discover effective tutoring approaches.

## ğŸ¯ Key Innovation

Traditional RLHF fine-tuning often leads to **mode collapse** â€” the model converges to a single repetitive teaching script. EPT solves this by:

1. **Evolving Structure**: Optimize the sequence of pedagogical actions (diagnose â†’ scaffold â†’ hint â†’ verify)
2. **Preserving Diversity**: Genetic algorithms maintain population diversity
3. **Separating Concerns**: The LLM generates text, but EPT controls the *strategy*

## ğŸ“Š Latest Results

EPT discovers hybrid teaching strategies that significantly outperform fixed baselines:

| Strategy | Genes | Fitness |
|----------|-------|---------|
| Direct Instruction | `[scaffold, scaffold, scaffold, scaffold]` | 10.0 |
| Chain-of-Thought | `[hint, hint, hint, hint]` | 56.7 |
| Verification Focus | `[verify, verify, verify, verify]` | 51.7 |
| **EPT Evolved** | **`[diagnose, verify, encourage, hint]`** | **93.3** |

**Improvement: +64.7% over best baseline (Chain-of-Thought)**

The evolved strategy follows a pedagogically sound approach: diagnose what the student knows â†’ verify their work â†’ encourage progress â†’ give targeted hints.

## âš¡ Performance Features

### Async API Calls
All LLM API calls run **concurrently** via `asyncio.gather()`. Instead of evaluating topologies one by one, the entire population is evaluated in parallel:

```
BEFORE: 60 sequential API calls â†’ ~500s per generation
AFTER:  60 concurrent API calls â†’ ~30s per generation (~4x speedup)
```

### Multi-Provider Fallback
The Classroom automatically chains multiple free LLM providers with automatic failover:

```
Groq (fastest, 14400 req/day) â†’ Cerebras (1M tokens/day) â†’ OpenRouter (fallback)
```

If a provider returns a rate limit error (HTTP 429), the next one is tried automatically. All providers use the **OpenAI-compatible API format**, so no code branching is needed.

| Provider | Teacher Model | Student Model | Free Tier |
|----------|--------------|---------------|-----------|
| **Groq** | `llama-3.3-70b-versatile` | `llama-3.3-70b-versatile` | 14,400 req/day |
| **Cerebras** | `qwen-3-32b` | `llama3.1-8b` | 1M tokens/day |
| **OpenRouter** | `llama-3.3-70b-instruct:free` | `qwen3-30b-a3b:free` | 1,000 req/day |

## ğŸ—ï¸ Project Structure

```
RESEARCH_WORK/
â”œâ”€â”€ ept/                    # Core EPT library
â”‚   â”œâ”€â”€ topology.py         # Genotype: Teaching strategy genes
â”‚   â”œâ”€â”€ classroom.py        # LLM orchestrator (async + multi-provider)
â”‚   â”œâ”€â”€ evolution.py        # Genetic algorithm (sync + async)
â”‚   â”œâ”€â”€ fitness.py          # Scoring function
â”‚   â””â”€â”€ utils.py            # Selection algorithms, diversity metrics
â”œâ”€â”€ mocks/                  # Lightweight mocks for heavy dependencies
â”‚   â”œâ”€â”€ torch_mock.py       # PyTorch mock (saves 2GB)
â”‚   â”œâ”€â”€ transformers_mock.py # HuggingFace mock (saves 500MB)
â”‚   â””â”€â”€ ...                 # vLLM, DeepSpeed, etc.
â”œâ”€â”€ config/
â”‚   â””â”€â”€ eval/
â”‚       â””â”€â”€ Qwen2.5-7B-Instruct.yaml  # Hydra config
â”œâ”€â”€ run_evolution.py        # Main entry point (async)
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ evolution_results.json  # Latest results
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv .venv
.venv\Scripts\activate        # Windows
# source .venv/bin/activate   # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure API Keys

Create a `.env` file with one or more provider keys:
```env
# Primary (fastest) â€” https://console.groq.com
GROQ_API_KEY=gsk_your-key-here

# Fallback â€” https://cloud.cerebras.ai
CEREBRAS_API_KEY=csk-your-key-here

# Last resort â€” https://openrouter.ai/keys
OPENROUTER_API_KEY=sk-or-your-key-here
```

At least one key is required. The more keys you add, the more resilient the system becomes.

### 3. Run Evolution

```bash
python run_evolution.py --config-name Qwen2.5-7B-Instruct.yaml
```

The console will show:
- Provider chain being used
- Per-generation timing
- Baseline vs evolved strategy comparison
- Results saved to `evolution_results.json`

## ğŸ§¬ How EPT Works

### Genotype (Topology)
A sequence of pedagogical actions â€” the "DNA" of a teaching strategy:
- `diagnose` â€” Ask what the student already knows
- `scaffold` â€” Break the problem into smaller steps
- `hint` â€” Give conceptual guidance without revealing the answer
- `verify` â€” Ask the student to check their work
- `encourage` â€” Positive reinforcement

### Phenotype (Conversation)
The LLM receives each action as a system instruction:
```
STRATEGY: Ask the student to verify their arithmetic.
PROBLEM: Solve for x: 3x + 12 = 27
```

### Fitness Function
```
+100   Student reaches correct answer
+30    Solved in â‰¤2 turns (efficiency bonus)
+15    Solved in â‰¤4 turns
 -15   Per teacher answer leak (penalty)
```

### Genetic Operators
- **Mutation**: Randomly change one action in the strategy
- **Crossover**: Combine two parent strategies at a random split
- **Selection**: Fitness-proportional (roulette wheel)
- **Elitism**: Best strategy preserved across generations

## ğŸ“ Configuration

Evolution parameters in `run_evolution.py`:
```python
EVOLUTION_CONFIG = {
    "population_size": 4,      # Individuals per generation
    "generations": 4,          # Evolutionary generations
    "gene_length": 4,          # Strategy length (number of actions)
    "max_turns": 5,            # Max conversation turns
    "mutation_rate": 0.6,      # Mutation vs crossover probability
    "elite_count": 1,          # Elites preserved per generation
}
```

## ï¿½ Lightweight Mock System (No GPU Required)

The mock system enables running on any machine without PyTorch, vLLM, or DeepSpeed:

| Mock | Replaced Library | Saves |
|------|------------------|-------|
| `torch_mock.py` | PyTorch | ~2GB |
| `transformers_mock.py` | HuggingFace Transformers | ~500MB |
| `vllm_mock.py` | vLLM inference engine | GPU requirement |
| `deepspeed_mock.py` | DeepSpeed | GPU requirement |

## ğŸ”¬ Research Roadmap

- [ ] Integrate GSM8K benchmark (50+ problems)
- [ ] Multi-run evaluation with statistical significance
- [ ] Ablation studies (mutation, crossover, population size)
- [ ] Robust evaluation (multi-sample averaging for noise reduction)
- [ ] Cross-model transfer analysis
- [ ] Human evaluation study

## ğŸ“„ License

MIT License â€” See LICENSE file for details.

## ğŸ™ Acknowledgments

- Based on the [pedagogicalrl](https://github.com/eth-lre/pedagogicalrl) framework
- LLM inference via [Groq](https://groq.com), [Cerebras](https://cerebras.ai), and [OpenRouter](https://openrouter.ai)
- Configuration management with [Hydra](https://hydra.cc/)
